{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oroz/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/oroz/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER: F\n",
      "train self.face_ids ['017', '018', '024', '030', '031', '032', '033', '035', '036', '037', '038', '040', '042', '043', '055', '056', '057', '058', '060', '061', '063', '064', '065', '067', '068', '068', '069', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '085', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '102', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '162', '163', '164', '165', '166', '167', '168', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '191', '192', '193', '194', '195', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '214', '215', '216', '217', '218', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '266', '267', '269', '270', '271', '272', '274', '275', '276', '277', '278', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '293', '294', '295', '297', '298', '299', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333']\n",
      "ENCODER: F\n",
      "val self.face_ids ['059', '070', '368', '369', '370', '371', '372', '373', '374', '375']\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oroz/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/oroz/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/oroz/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "from data_processing.data_module import FaceDataModule\n",
    "from data_processing.dataset import FaceDataset\n",
    "from model.gauss_predictor import GaussPredictor\n",
    "import yaml\n",
    "import k3d\n",
    "from gaussian_splatting.utils.sh_utils import SH2RGB\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "\n",
    "config = yaml.safe_load(open('configs/settings_final.yaml', 'r'))\n",
    "config['data']['force_rebuild'] = False\n",
    "config['data']['save_results'] = False\n",
    "\n",
    "face_data_module = FaceDataModule(config=config)\n",
    "face_data_module.setup('fit')\n",
    "\n",
    "face_data_loader_val = face_data_module.val_dataloader()\n",
    "\n",
    "gauss_predictor = GaussPredictor.load_from_checkpoint(config['model']['checkpoint'], config=config, data_module=face_data_module)\n",
    "gauss_predictor = gauss_predictor.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/oroz/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/pytorch_lightning/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      " 20%|██        | 2/10 [00:23<01:33, 11.68s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 9.77 GiB of which 2.38 MiB is free. Process 359245 has 6.98 GiB memory in use. Including non-PyTorch memory, this process has 2.76 GiB memory in use. Of the allocated memory 2.38 GiB is allocated by PyTorch, and 84.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m\n\u001b[1;32m     43\u001b[0m rendered_images, rendered_depth \u001b[38;5;241m=\u001b[39m gauss_predictor\u001b[38;5;241m.\u001b[39mrender_images(\n\u001b[1;32m     44\u001b[0m     gaussian_model\u001b[38;5;241m=\u001b[39mgaussian_model,\n\u001b[1;32m     45\u001b[0m     camera_parameters\u001b[38;5;241m=\u001b[39mcamera_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m rendered_images \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m rendered_images\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 53\u001b[0m lpips_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgauss_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_lpips_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrendered_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     54\u001b[0m ssim_loss \u001b[38;5;241m=\u001b[39m gauss_predictor\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mcompute_ssim_loss(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, rendered_images, images)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     55\u001b[0m psnr_loss \u001b[38;5;241m=\u001b[39m compute_psnr_loss(rendered_images, images)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/gaussian-splatting/3DSSL-SS24_Generalizable3DHeadReconstruction/losses/losses.py:108\u001b[0m, in \u001b[0;36mLosses.compute_lpips_loss\u001b[0;34m(self, data_split, rendered_images, target_images)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m serial, rendered_image \u001b[38;5;129;01min\u001b[39;00m rendered_images\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    107\u001b[0m     target_image \u001b[38;5;241m=\u001b[39m target_images[serial]\n\u001b[0;32m--> 108\u001b[0m     lpips_loss_for_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlpips\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrendered_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     lpips_losses\u001b[38;5;241m.\u001b[39mappend(lpips_loss_for_image\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    111\u001b[0m lpips_losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(lpips_losses, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgauss_predictor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/lpips/lpips.py:124\u001b[0m, in \u001b[0;36mLPIPS.forward\u001b[0;34m(self, in0, in1, retPerLayer, normalize)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL):\n\u001b[1;32m    123\u001b[0m     feats0[kk], feats1[kk] \u001b[38;5;241m=\u001b[39m lpips\u001b[38;5;241m.\u001b[39mnormalize_tensor(outs0[kk]), lpips\u001b[38;5;241m.\u001b[39mnormalize_tensor(outs1[kk])\n\u001b[0;32m--> 124\u001b[0m     diffs[kk] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mfeats0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mfeats1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlpips):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial):\n",
      "File \u001b[0;32m~/anaconda3/envs/gaussian_splatting/lib/python3.8/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 9.77 GiB of which 2.38 MiB is free. Process 359245 has 6.98 GiB memory in use. Including non-PyTorch memory, this process has 2.76 GiB memory in use. Of the allocated memory 2.38 GiB is allocated by PyTorch, and 84.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "def compute_psnr_loss(rendered_images, target_images):\n",
    "    losses = []\n",
    "    for serial, rendered_image in rendered_images.items():\n",
    "        psnr = PeakSignalNoiseRatio(data_range=1.0).cuda()\n",
    "        target_image = target_images[serial]\n",
    "        losses.append(psnr(rendered_image.unsqueeze(0), target_image.unsqueeze(0)).unsqueeze(0))\n",
    "    \n",
    "    losses = torch.cat(losses, dim=0).to(gauss_predictor.device)\n",
    "    return losses\n",
    "    \n",
    "\n",
    "ids = []\n",
    "lpips_scores = []\n",
    "psnr_scores = []\n",
    "ssim_scores = []\n",
    "\n",
    "for batch in tqdm(face_data_loader_val):\n",
    "    batch = face_data_module.transfer_batch_to_device(batch, 'cpu', 0)\n",
    "    initial_gaussians = batch['initial_gaussians']\n",
    "    images = batch['images']\n",
    "    target_depths = batch['depths']\n",
    "    sequence_id = batch['sequence_id']\n",
    "    \n",
    "    camera_params = batch['camera_params']\n",
    "    camera_params['cam_2_world_poses'] = FaceDataset.dicts_to_poses(camera_params['cam_2_world_poses'])\n",
    "    \n",
    "    input_serials = batch['input_serials']\n",
    "    input_cam_2_world_poses = {serial: cam_2_world_pose for serial, cam_2_world_pose in camera_params['cam_2_world_poses'].items() if serial in input_serials}\n",
    "    input_camera_params = {'cam_2_world_poses': input_cam_2_world_poses, 'intrinsics': camera_params['intrinsics']}\n",
    "    \n",
    "    gaussian_model = gauss_predictor.get_gaussian_model(sequence_id)\n",
    "    \n",
    "    predicted_gaussians = gauss_predictor.forward(initial_gaussians)\n",
    "    predicted_gaussians = {key: val.cuda() for key, val in predicted_gaussians.items()}\n",
    "    gaussian_model.set_gaussian_properties(predicted_gaussians)\n",
    "    \n",
    "    first_image = list(images.values())[0]\n",
    "    _, image_height, image_width = first_image.shape\n",
    "    \n",
    "    rendered_images, rendered_depth = gauss_predictor.render_images(\n",
    "        gaussian_model=gaussian_model,\n",
    "        camera_parameters=camera_params,\n",
    "        height=image_height,\n",
    "        width=image_width,\n",
    "        device='cuda'\n",
    "    )\n",
    "    \n",
    "    rendered_images = {key: val.cpu() for key, val in rendered_images.items()}\n",
    "    \n",
    "    lpips_loss = gauss_predictor.losses.compute_lpips_loss('val', rendered_images, images).mean()\n",
    "    ssim_loss = gauss_predictor.losses.compute_ssim_loss('val', rendered_images, images).mean()\n",
    "    psnr_loss = compute_psnr_loss(rendered_images, images).mean()\n",
    "    \n",
    "    lpips_scores.append(lpips_loss)\n",
    "    ssim_scores.append(ssim_loss)\n",
    "    psnr_scores.append(psnr_loss)\n",
    "    \n",
    "    id=sequence_id.replace('/EXP-1-head','')\n",
    "    ids.append(id)\n",
    "    assert id in gauss_predictor.test_ids\n",
    "    \n",
    "    \n",
    "print(f\"{'ID'<10} {'LPIPS':<10} {'SSIM':<10} {'PSNR':<10}\")\n",
    "for i, id in enumerate(ids):\n",
    "    print(f\"{id<10} {lpips_scores[i]:.04f<10} {ssim_scores[i]:.04f<10} {psnr_scores[i]:.04f<10}\")\n",
    "\n",
    "print('')\n",
    "print(f\"{'all'<10} {mean(lpips_scores):.04f<10} {mean(ssim_scores):.04f<10} {mean(psnr_scores):.04f<10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaussian_splatting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
